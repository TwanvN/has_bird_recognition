{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1517297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "os.environ[\"TF_AUTOTUNE_THRESHOLD\"] = \"3\"\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "os.environ[\"TF_ENABLE_XLA\"] = \"true\"\n",
    "os.environ['TF_USE_CUDNN_BATCHNORM_SPATIAL_PERSISTENT'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "# import librosa.display\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models, layers\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "import functools\n",
    "\n",
    "# settings\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.set_logical_device_configuration(physical_devices[0],[tf.config.LogicalDeviceConfiguration(memory_limit=3500)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73695e0a",
   "metadata": {},
   "source": [
    "<h1>Audio ML with multiple features</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0a0ac2b",
   "metadata": {},
   "source": [
    "<h2>Loading the Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e71c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = \"/home/birdo/MachineLearning/Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caeac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(basePath + \"bird_sounds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e382620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315022c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.species_id.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72d61846",
   "metadata": {},
   "source": [
    "## Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "allImPath = basePath + \"Images-512/all/\"\n",
    "allLabelPath = basePath + \"Labels/all/\"\n",
    "valid_files = []\n",
    "labels = []\n",
    "for img_file in os.listdir(allImPath):\n",
    "    if os.path.isfile(allImPath + img_file) and os.path.isfile(allLabelPath + img_file):\n",
    "        if os.path.isfile(allLabelPath + img_file):\n",
    "            valid_files.append(img_file)\n",
    "            f = open(allLabelPath + img_file, \"r\")\n",
    "            label = f.read()\n",
    "            f.close()\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(\"No label could be found of: \" + img_file)\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    valid_files, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    ")  # 0.25 x 0.8 = 0.2\n",
    "print(len(y_train), len(y_test), len(y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e9f2933",
   "metadata": {},
   "source": [
    "## Prepare data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55184b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DCT_OUTPUTS = 32\n",
    "MFCC_SIZE = 623\n",
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sound_Generator(keras.utils.Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size, directory):\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.imBasePath = basePath + str(\"Images-512/\") + directory\n",
    "        self.labelBasePath = basePath + str(\"Labels/\") + directory\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(8)\n",
    "        random.seed(42)\n",
    "\n",
    "    def __get_all_labels__(self):\n",
    "        y = np.asarray(self.labels, dtype=np.float32)\n",
    "        return y\n",
    "\n",
    "    def __on_epoch_end(self):\n",
    "        c = list(zip(self.image_filenames, self.labels))\n",
    "        random.shuffle(c)\n",
    "        self.image_filenames, self.labels = zip(*c)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(\n",
    "            np.int\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_filenames[\n",
    "            idx * self.batch_size : (idx + 1) * self.batch_size\n",
    "        ]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        X = self.get_images_data(batch_x, self.imBasePath)\n",
    "\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        y0 = np.asarray(batch_y, dtype=np.float32)\n",
    "\n",
    "        return X, y0\n",
    "\n",
    "    def get_image_data(self,file_path):\n",
    "        _data = np.fromfile(file_path, dtype=np.float64)\n",
    "        return _data.reshape((MFCC_SIZE, NUM_DCT_OUTPUTS, CHANNELS))\n",
    "\n",
    "    def get_images_data(self,_batch_x, _imBasePath):\n",
    "        tasks = [\n",
    "            self.executor.submit(self.get_image_data,_imBasePath + _image_f)\n",
    "            for _image_f in _batch_x\n",
    "        ]\n",
    "        return [task.result() for task in tasks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6771fd6a",
   "metadata": {},
   "source": [
    "## Load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_test = X_test[15000]\n",
    "print(X_test_test)\n",
    "image = np.fromfile((allImPath + X_test_test), dtype=np.float64).reshape(\n",
    "    (MFCC_SIZE,NUM_DCT_OUTPUTS, CHANNELS)\n",
    ")\n",
    "print(image.shape)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "train_batch_generator = Sound_Generator(X_train, y_train, batch_size, \"all/\")\n",
    "val_batch_generator = Sound_Generator(X_val, y_val, batch_size, \"all/\")\n",
    "test_batch_generator = Sound_Generator(X_test, y_test, batch_size, \"all/\")\n",
    "mini_train_batch_generator = Sound_Generator(\n",
    "    X_train[:400], y_train[:400], batch_size, \"all/\"\n",
    ")\n",
    "mini_test_batch_generator = Sound_Generator(\n",
    "    X_val[:100], y_val[:100], batch_size, \"all/\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8821463",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1649ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (MFCC_SIZE, NUM_DCT_OUTPUTS, CHANNELS)\n",
    "n_classes = df.species_id.unique().shape[0]\n",
    "CNNmodel = models.Sequential()\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(16, kernel_size=(3,3),input_shape=input_shape, activation='tanh', padding='same', name='conv2d_tanh', kernel_regularizer=l2(0.0001)))\n",
    "CNNmodel.add(layers.MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_1'))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same', name='conv2d_relu_1',kernel_regularizer=l2(0.0001)))\n",
    "CNNmodel.add(layers.MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_2'))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(32, kernel_size=(3,3), activation='relu', padding='same', name='conv2d_relu_2',kernel_regularizer=l2(0.0001)))\n",
    "CNNmodel.add(layers.MaxPooling2D(pool_size=(2,2), padding='same', name='max_pool_2d_3'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_3'))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', name='conv2d_relu_3',kernel_regularizer=l2(0.0001)))\n",
    "CNNmodel.add(layers.MaxPooling2D(pool_size=(2,2), padding='same', name='max_pool_2d_4'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_4'))\n",
    "\n",
    "CNNmodel.add(layers.Conv2D(64, kernel_size=(3,3), activation='relu', padding='same', name='conv2d_relu_4', kernel_regularizer=l2(0.0001)))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_5'))\n",
    "CNNmodel.add(layers.Flatten(name='flatten'))\n",
    "\n",
    "CNNmodel.add(layers.Dense(64, activation='relu', activity_regularizer=l2(0.0001),kernel_regularizer=l2(0.0001), name='dense'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_6'))\n",
    "CNNmodel.add(layers.Dense(32, activation=\"relu\", name='dense_2'))\n",
    "CNNmodel.add(layers.Dropout(rate=0.1, name='dropout_7'))\n",
    "CNNmodel.add(layers.Dense(n_classes, activation='softmax', name='softmax'))\n",
    "CNNmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = True\n",
    "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
    "if load_model:\n",
    "    CNNmodel = tf.keras.models.load_model(basePath + str(\"/Models/MFCCOnly\"))\n",
    "    # set learning rate\n",
    "    CNNmodel.optimizer.learning_rate = 0.0001\n",
    "else:\n",
    "    CNNmodel.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=[\"accuracy\"],\n",
    "        jit_compile=True,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b406fc4",
   "metadata": {},
   "source": [
    "## Define tensorflow callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = basePath + \"logs/MFCCOnly/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,profile_batch='600, 620')\n",
    "file_writer_cm = tf.summary.create_file_writer(log_dir + \"/cm\")\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=basePath + str(\"/Models/MFCCOnly\"),\n",
    "    save_weights_only=False,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c230b5be",
   "metadata": {},
   "source": [
    "### Define Confusion Matrix callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "\n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    \n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.0\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55497983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"\n",
    "    Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\n",
    "    \"\"\"\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "\n",
    "    # Use plt.savefig to save the plot to a PNG in memory.\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Use tf.image.decode_png to convert the PNG buffer\n",
    "    # to a TF image. Make sure you use 4 channels.\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "\n",
    "    # Use tf.expand_dims to add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb09a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_confusion_matrix(epoch, logs):\n",
    "    batch_generator = val_batch_generator\n",
    "\n",
    "    figure = generate_cm(batch_generator)\n",
    "    cm_image = plot_to_image(figure)\n",
    "\n",
    "    # Log the confusion matrix as an image summary.\n",
    "    with file_writer_cm.as_default():\n",
    "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cm(generator):\n",
    "    y_pred_raw = CNNmodel.predict(generator)\n",
    "\n",
    "    \n",
    "    y_pred = np.argmax(y_pred_raw, axis=1)\n",
    "    y_true = generator.__get_all_labels__()\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    figure = plot_confusion_matrix(cm, class_names=[\"Wilde Eend\", \"Tjiftjaf\", \"Koolmees\", \"Houtduif\", \"Huismus\", \"Merel\", \"Winterkoning\", \"Fitis\", \"Vink\", \"Spreeuw\", \"Geen Vogel\"])\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35c6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_callback = keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5c7a7ca",
   "metadata": {},
   "source": [
    "## Calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for specie in df.species_id.unique():\n",
    "    amount = len(df[df[\"species_id\"] == specie])\n",
    "    weight = (1 / amount) * (len(df) / len(df.species_id.unique()))\n",
    "    weights.append(weight)\n",
    "zip_weights = zip(range(0, len(df.species_id.unique())), weights)\n",
    "class_weights = {}\n",
    "for (\n",
    "    i,\n",
    "    w,\n",
    ") in zip_weights:\n",
    "    class_weights[i] = w\n",
    "print(class_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbe3d090",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = train_batch_generator\n",
    "val_gen = val_batch_generator\n",
    "\n",
    "CNNmodel.fit(\n",
    "    train_gen,\n",
    "    epochs=256,\n",
    "    verbose=1,\n",
    "    validation_data=val_gen,\n",
    "    callbacks=[tensorboard_callback, model_checkpoint_callback, cm_callback],\n",
    "    class_weight=class_weights,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f35a8be8",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2541822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = CNNmodel.evaluate(test_batch_generator, verbose=1)\n",
    "print(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n",
    "\n",
    "conf_matrix = generate_cm(test_batch_generator)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(CNNmodel)\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "converter.optimizations = {tf.lite.Optimize.DEFAULT}\n",
    "# set too support only float32 input and output\n",
    "converter.target_spec.supported_types = [tf.float32]\n",
    "\n",
    "\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "# print size of model\n",
    "print(\"Size of model: \", len(tflite_model) / 1024, \" kb\")\n",
    "\n",
    "with open(basePath+'model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
